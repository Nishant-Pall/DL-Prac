{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit"
  },
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"KERAS_BACKEND\"] = \"plaidml.keras.backend\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import LSTM, GRU, Input, Dense, Embedding\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONFIG\n",
    "BATCH_SIZE = 64\n",
    "LATENT_DIM = 256\n",
    "EPOCHS = 100\n",
    "NUM_SAMPLES = 10000\n",
    "MAX_NUM_WORDS = 20000\n",
    "EMBEDDING_DIM = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_texts = []\n",
    "target_texts = []\n",
    "target_texts_inputs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "t = 0\n",
    "for line in open(\"spa.txt\", encoding=\"utf-8\"):\n",
    "    t += 1\n",
    "    if t > NUM_SAMPLES:\n",
    "        break\n",
    "    if '\\t' not in line:\n",
    "        continue\n",
    "    input_text, translation, *rest = line.rstrip().split(\"\\t\")\n",
    "    target_text = translation + ' <eos>'\n",
    "    target_text_input = '<sos> ' + translation\n",
    "\n",
    "    input_texts.append(input_text)\n",
    "    target_texts.append(target_text)\n",
    "    target_texts_inputs.append(target_text_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "metadata": {},
     "execution_count": 65
    }
   ],
   "source": [
    "len(input_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_inputs = Tokenizer(num_words=MAX_NUM_WORDS)\n",
    "tokenizer_inputs.fit_on_texts(input_texts)\n",
    "input_sequences = tokenizer_inputs.texts_to_sequences(input_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "2355"
      ]
     },
     "metadata": {},
     "execution_count": 67
    }
   ],
   "source": [
    "word2idx_input = tokenizer_inputs.word_index\n",
    "len(word2idx_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_outputs = Tokenizer(num_words=MAX_NUM_WORDS, filters='')\n",
    "tokenizer_outputs.fit_on_texts(\n",
    "    target_texts + target_texts_inputs)  # inefficient, oh well\n",
    "target_sequences = tokenizer_outputs.texts_to_sequences(target_texts)\n",
    "target_sequences_inputs = tokenizer_outputs.texts_to_sequences(\n",
    "    target_texts_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "6326"
      ]
     },
     "metadata": {},
     "execution_count": 69
    }
   ],
   "source": [
    "word2idx_output = tokenizer_outputs.word_index\n",
    "len(word2idx_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "max input length: 5\nmax output length: 9\n"
     ]
    }
   ],
   "source": [
    "max_len_input = max(len(s) for s in input_sequences)\n",
    "max_len_output = max(len(s) for s in target_sequences)\n",
    "\n",
    "print(f'max input length: {max_len_input}')\n",
    "print(f'max output length: {max_len_output}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_inputs = pad_sequences(input_sequences, maxlen=max_len_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(10000, 5)"
      ]
     },
     "metadata": {},
     "execution_count": 72
    }
   ],
   "source": [
    "encoder_inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_inputs = pad_sequences(target_sequences_inputs, maxlen=max_len_output, padding='post')\n",
    "decoder_outputs = pad_sequences(target_sequences, maxlen=max_len_output, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(10000, 9)"
      ]
     },
     "metadata": {},
     "execution_count": 74
    }
   ],
   "source": [
    "decoder_outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(10000, 9)"
      ]
     },
     "metadata": {},
     "execution_count": 75
    }
   ],
   "source": [
    "decoder_inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec = {}\n",
    "\n",
    "with open(os.path.join(\"glove.6B.100d.txt\")) as f:\n",
    "    for lines in f:\n",
    "        values = lines.split()\n",
    "        word = values[0]\n",
    "        vec = np.asarray(values[1:], dtype='float32')\n",
    "        word2vec[word] = vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "400000"
      ]
     },
     "metadata": {},
     "execution_count": 78
    }
   ],
   "source": [
    "len(word2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "2356"
      ]
     },
     "metadata": {},
     "execution_count": 79
    }
   ],
   "source": [
    "num_words = min(MAX_NUM_WORDS, len(word2idx_input) + 1)\n",
    "num_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "for word, i in word2idx_input.items():\n",
    "    if i < MAX_NUM_WORDS:\n",
    "        embedding_vector = word2vec.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([-0.98792  ,  0.70872  ,  0.16251  , -0.10026  ,  0.58269  ,\n",
       "        0.073669 ,  0.3122   ,  0.44948  ,  0.35266  , -0.34202  ,\n",
       "        0.86923  , -0.2635   ,  0.18212  , -0.094346 ,  0.58245  ,\n",
       "       -0.32286  ,  0.5095   ,  0.2932   , -0.56824  ,  0.20888  ,\n",
       "       -0.41607  , -0.51531  ,  0.088144 ,  0.32069  , -0.13685  ,\n",
       "       -0.25164  ,  0.57618  , -0.40587  , -0.58642  ,  0.51108  ,\n",
       "        0.18728  ,  0.45255  , -0.96556  ,  0.11442  ,  1.0369   ,\n",
       "        1.2553   , -0.56367  ,  0.31116  , -0.15092  , -0.70328  ,\n",
       "        0.44437  , -0.20229  , -0.71858  ,  0.071706 ,  0.12639  ,\n",
       "       -0.052942 ,  0.078235 , -0.85217  ,  0.10476  , -0.53999  ,\n",
       "        0.56716  ,  0.11658  ,  0.060324 ,  0.53872  ,  0.16038  ,\n",
       "       -1.6095   , -0.55631  , -0.48165  ,  2.27     , -0.0043802,\n",
       "       -0.43512  ,  0.58191  , -0.095824 ,  0.32041  ,  0.88939  ,\n",
       "       -0.25354  ,  0.07366  , -0.24358  ,  0.58028  , -0.27117  ,\n",
       "        0.0091611, -0.10121  ,  0.35215  , -0.065269 ,  0.5111   ,\n",
       "        0.45435  ,  0.060997 ,  0.1772   , -0.90978  , -0.56311  ,\n",
       "        0.35152  , -0.64104  , -0.30783  , -0.32654  , -1.469    ,\n",
       "        0.28003  ,  0.47381  , -0.075506 , -0.4421   ,  0.14936  ,\n",
       "       -0.38218  ,  0.17083  ,  0.35604  , -0.31559  , -0.79628  ,\n",
       "        0.17794  , -0.14377  ,  0.066019 ,  1.2434   ,  0.13377  ],\n",
       "      dtype=float32)"
      ]
     },
     "metadata": {},
     "execution_count": 83
    }
   ],
   "source": [
    "embedding_layer = Embedding(\n",
    "    num_words,\n",
    "    EMBEDDING_DIM,\n",
    "    weights=[embedding_matrix],\n",
    "    input_length=max_len_input\n",
    ")"
   ]
  }
 ]
}