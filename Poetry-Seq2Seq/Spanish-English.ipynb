{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit"
  },
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"KERAS_BACKEND\"] = \"plaidml.keras.backend\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<module 'keras.backend_config' from '/home/prhyme/.local/lib/python3.8/site-packages/keras/backend_config.py'>"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "keras.backend_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import LSTM, GRU, Input, Dense, Embedding\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONFIG\n",
    "BATCH_SIZE = 64\n",
    "LATENT_DIM = 128\n",
    "EPOCHS = 100\n",
    "NUM_SAMPLES = 10000\n",
    "MAX_NUM_WORDS = 20000\n",
    "EMBEDDING_DIM = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_texts = []\n",
    "target_texts = []\n",
    "target_texts_inputs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "t = 0\n",
    "for line in open(\"spa.txt\", encoding=\"utf-8\"):\n",
    "    t += 1\n",
    "    if t > NUM_SAMPLES:\n",
    "        break\n",
    "    if '\\t' not in line:\n",
    "        continue\n",
    "    input_text, translation, *rest = line.rstrip().split(\"\\t\")\n",
    "    target_text = translation + ' <eos>'\n",
    "    target_text_input = '<sos> ' + translation\n",
    "\n",
    "    input_texts.append(input_text)\n",
    "    target_texts.append(target_text)\n",
    "    target_texts_inputs.append(target_text_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "len(input_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_inputs = Tokenizer(num_words=MAX_NUM_WORDS)\n",
    "tokenizer_inputs.fit_on_texts(input_texts)\n",
    "input_sequences = tokenizer_inputs.texts_to_sequences(input_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "2355"
      ]
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "source": [
    "word2idx_input = tokenizer_inputs.word_index\n",
    "len(word2idx_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_outputs = Tokenizer(num_words=MAX_NUM_WORDS, filters='')\n",
    "tokenizer_outputs.fit_on_texts(\n",
    "    target_texts + target_texts_inputs)  # inefficient, oh well\n",
    "target_sequences = tokenizer_outputs.texts_to_sequences(target_texts)\n",
    "target_sequences_inputs = tokenizer_outputs.texts_to_sequences(\n",
    "    target_texts_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "6326"
      ]
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "source": [
    "word2idx_output = tokenizer_outputs.word_index\n",
    "len(word2idx_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "6327"
      ]
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "source": [
    "num_words_output = len(word2idx_output) + 1\n",
    "num_words_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "max input length: 5\nmax output length: 9\n"
     ]
    }
   ],
   "source": [
    "max_len_input = max(len(s) for s in input_sequences)\n",
    "max_len_output = max(len(s) for s in target_sequences)\n",
    "\n",
    "print(f'max input length: {max_len_input}')\n",
    "print(f'max output length: {max_len_output}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_inputs = pad_sequences(input_sequences, maxlen=max_len_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(10000, 5)"
      ]
     },
     "metadata": {},
     "execution_count": 29
    }
   ],
   "source": [
    "encoder_inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_inputs = pad_sequences(target_sequences_inputs, maxlen=max_len_output, padding='post')\n",
    "decoder_outputs = pad_sequences(target_sequences, maxlen=max_len_output, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(10000, 9)"
      ]
     },
     "metadata": {},
     "execution_count": 31
    }
   ],
   "source": [
    "decoder_outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(10000, 9)"
      ]
     },
     "metadata": {},
     "execution_count": 32
    }
   ],
   "source": [
    "decoder_inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec = {}\n",
    "\n",
    "with open(os.path.join(\"glove.6B.100d.txt\")) as f:\n",
    "    for lines in f:\n",
    "        values = lines.split()\n",
    "        word = values[0]\n",
    "        vec = np.asarray(values[1:], dtype='float32')\n",
    "        word2vec[word] = vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "400000"
      ]
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "source": [
    "len(word2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "2356"
      ]
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "source": [
    "num_words = min(MAX_NUM_WORDS, len(word2idx_input) + 1)\n",
    "num_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "for word, i in word2idx_input.items():\n",
    "    if i < MAX_NUM_WORDS:\n",
    "        embedding_vector = word2vec.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = Embedding(\n",
    "    num_words,\n",
    "    EMBEDDING_DIM,\n",
    "    weights=[embedding_matrix],\n",
    "    input_length=max_len_input\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_targets_one_hot = np.zeros((\n",
    "    len(input_texts),\n",
    "    max_len_output,\n",
    "    num_words_output),\n",
    "    dtype='float32'\n",
    ")\n",
    "\n",
    "for i, d in enumerate(decoder_outputs):\n",
    "    for t, word in enumerate(d):\n",
    "        decoder_targets_one_hot[i, t, word] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(10000, 9, 6327)"
      ]
     },
     "metadata": {},
     "execution_count": 36
    }
   ],
   "source": [
    "decoder_targets_one_hot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(10000, 9)"
      ]
     },
     "metadata": {},
     "execution_count": 47
    }
   ],
   "source": [
    "decoder_outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BUILD THE MODEL\n",
    "\n",
    "encoder_inputs_placeholder = Input(shape=(max_len_input,))\n",
    "X = embedding_layer(encoder_inputs_placeholder)\n",
    "encoder = LSTM(\n",
    "    LATENT_DIM,\n",
    "    return_state=True,\n",
    "    dropout=0.5\n",
    ")\n",
    "encoder_outputs, h, c = encoder(X)\n",
    "encoder_states = [h, c]\n",
    "\n",
    "decoder_inputs_placeholder = Input(shape=(max_len_output,))\n",
    "decoder_embedding = Embedding(num_words_output, EMBEDDING_DIM)\n",
    "decoder_inputs_x = decoder_embedding(decoder_inputs_placeholder)\n",
    "\n",
    "decoder_LSTM = LSTM(LATENT_DIM, return_sequences=True, dropout=0.5, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_LSTM(decoder_inputs_x, initial_state=encoder_states)\n",
    "\n",
    "decoder_dense = Dense(num_words_output, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model([encoder_inputs_placeholder, decoder_inputs_placeholder], decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def custom_loss(y_true, y_pred):\n",
    "#     # both are of shape N x T x K\n",
    "#     mask = K.cast(y_true > 0, dtype='float32')\n",
    "#     out = mask * y_true * K.log(y_pred)\n",
    "#     return -K.sum(out) / K.sum(mask)\n",
    "\n",
    "\n",
    "# def acc(y_true, y_pred):\n",
    "#     # both are of shape N x T x K\n",
    "#     targ = K.argmax(y_true, axis=-1)\n",
    "#     pred = K.argmax(y_pred, axis=-1)\n",
    "#     correct = K.cast(K.equal(targ, pred), dtype='float32')\n",
    "\n",
    "#     # 0 is padding, don't include those\n",
    "#     mask = K.cast(K.greater(targ, 0), dtype='float32')\n",
    "#     n_correct = K.sum(mask * correct)\n",
    "#     n_total = K.sum(mask)\n",
    "#     return n_correct / n_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer='rmsprop',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['acc']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/100\n",
      "125/125 [==============================] - 55s 231ms/step - loss: 3.4605 - acc: 0.5926 - val_loss: 2.6650 - val_acc: 0.6473\n",
      "Epoch 2/100\n",
      "125/125 [==============================] - 26s 209ms/step - loss: 2.0930 - acc: 0.7127 - val_loss: 2.4740 - val_acc: 0.6667\n",
      "Epoch 3/100\n",
      "125/125 [==============================] - 25s 197ms/step - loss: 1.9007 - acc: 0.7259 - val_loss: 2.3535 - val_acc: 0.6823\n",
      "Epoch 4/100\n",
      "125/125 [==============================] - 25s 201ms/step - loss: 1.7621 - acc: 0.7393 - val_loss: 2.2322 - val_acc: 0.6976\n",
      "Epoch 5/100\n",
      "125/125 [==============================] - 26s 211ms/step - loss: 1.6592 - acc: 0.7502 - val_loss: 2.1562 - val_acc: 0.7098\n",
      "Epoch 6/100\n",
      "125/125 [==============================] - 26s 209ms/step - loss: 1.5402 - acc: 0.7645 - val_loss: 2.0763 - val_acc: 0.7188\n",
      "Epoch 7/100\n",
      "125/125 [==============================] - 26s 210ms/step - loss: 1.4649 - acc: 0.7706 - val_loss: 2.0459 - val_acc: 0.7281\n",
      "Epoch 8/100\n",
      "125/125 [==============================] - 26s 210ms/step - loss: 1.3994 - acc: 0.7778 - val_loss: 1.9989 - val_acc: 0.7322\n",
      "Epoch 9/100\n",
      "125/125 [==============================] - 26s 209ms/step - loss: 1.3251 - acc: 0.7854 - val_loss: 2.0017 - val_acc: 0.7346\n",
      "Epoch 10/100\n",
      "125/125 [==============================] - 26s 211ms/step - loss: 1.2687 - acc: 0.7933 - val_loss: 1.9715 - val_acc: 0.7392\n",
      "Epoch 11/100\n",
      "125/125 [==============================] - 26s 212ms/step - loss: 1.2317 - acc: 0.7961 - val_loss: 1.9568 - val_acc: 0.7442\n",
      "Epoch 12/100\n",
      "125/125 [==============================] - 26s 211ms/step - loss: 1.1846 - acc: 0.8011 - val_loss: 1.9217 - val_acc: 0.7469\n",
      "Epoch 13/100\n",
      "125/125 [==============================] - 26s 206ms/step - loss: 1.1525 - acc: 0.8063 - val_loss: 1.9271 - val_acc: 0.7469\n",
      "Epoch 14/100\n",
      "125/125 [==============================] - 23s 185ms/step - loss: 1.1192 - acc: 0.8091 - val_loss: 1.9171 - val_acc: 0.7462\n",
      "Epoch 15/100\n",
      "125/125 [==============================] - 25s 199ms/step - loss: 1.0874 - acc: 0.8157 - val_loss: 1.9478 - val_acc: 0.7446\n",
      "Epoch 16/100\n",
      "125/125 [==============================] - 26s 212ms/step - loss: 1.0577 - acc: 0.8195 - val_loss: 1.9264 - val_acc: 0.7446\n",
      "Epoch 17/100\n",
      "125/125 [==============================] - 27s 213ms/step - loss: 1.0394 - acc: 0.8219 - val_loss: 1.9216 - val_acc: 0.7473\n",
      "Epoch 18/100\n",
      "125/125 [==============================] - 25s 201ms/step - loss: 1.0057 - acc: 0.8263 - val_loss: 1.9194 - val_acc: 0.7436\n",
      "Epoch 19/100\n",
      "125/125 [==============================] - 22s 177ms/step - loss: 0.9762 - acc: 0.8285 - val_loss: 1.9083 - val_acc: 0.7441\n",
      "Epoch 20/100\n",
      "125/125 [==============================] - 22s 176ms/step - loss: 0.9510 - acc: 0.8339 - val_loss: 1.9284 - val_acc: 0.7436\n",
      "Epoch 21/100\n",
      "125/125 [==============================] - 22s 177ms/step - loss: 0.9338 - acc: 0.8356 - val_loss: 1.9140 - val_acc: 0.7414\n",
      "Epoch 22/100\n",
      "125/125 [==============================] - 22s 176ms/step - loss: 0.9228 - acc: 0.8366 - val_loss: 1.9246 - val_acc: 0.7416\n",
      "Epoch 23/100\n",
      "125/125 [==============================] - 22s 175ms/step - loss: 0.8987 - acc: 0.8417 - val_loss: 1.9362 - val_acc: 0.7387\n",
      "Epoch 24/100\n",
      "125/125 [==============================] - 21s 171ms/step - loss: 0.8814 - acc: 0.8435 - val_loss: 1.9392 - val_acc: 0.7370\n",
      "Epoch 25/100\n",
      "125/125 [==============================] - 20s 158ms/step - loss: 0.8675 - acc: 0.8461 - val_loss: 1.9609 - val_acc: 0.7353\n",
      "Epoch 26/100\n",
      "125/125 [==============================] - 20s 164ms/step - loss: 0.8429 - acc: 0.8508 - val_loss: 1.9648 - val_acc: 0.7354\n",
      "Epoch 27/100\n",
      "125/125 [==============================] - 19s 156ms/step - loss: 0.8395 - acc: 0.8504 - val_loss: 1.9705 - val_acc: 0.7359\n",
      "Epoch 28/100\n",
      "125/125 [==============================] - 18s 146ms/step - loss: 0.8244 - acc: 0.8513 - val_loss: 1.9807 - val_acc: 0.7353\n",
      "Epoch 29/100\n",
      "125/125 [==============================] - 25s 201ms/step - loss: 0.8003 - acc: 0.8558 - val_loss: 1.9785 - val_acc: 0.7358\n",
      "Epoch 30/100\n",
      "125/125 [==============================] - 21s 169ms/step - loss: 0.7782 - acc: 0.8591 - val_loss: 2.0019 - val_acc: 0.7348\n",
      "Epoch 31/100\n",
      "125/125 [==============================] - 23s 186ms/step - loss: 0.7769 - acc: 0.8594 - val_loss: 2.0069 - val_acc: 0.7348\n",
      "Epoch 32/100\n",
      "125/125 [==============================] - 18s 146ms/step - loss: 0.7720 - acc: 0.8606 - val_loss: 2.0216 - val_acc: 0.7346\n",
      "Epoch 33/100\n",
      "125/125 [==============================] - 23s 185ms/step - loss: 0.7650 - acc: 0.8615 - val_loss: 2.0296 - val_acc: 0.7342\n",
      "Epoch 34/100\n",
      "125/125 [==============================] - 25s 198ms/step - loss: 0.7498 - acc: 0.8635 - val_loss: 2.0408 - val_acc: 0.7329\n",
      "Epoch 35/100\n",
      "125/125 [==============================] - 30s 242ms/step - loss: 0.7393 - acc: 0.8663 - val_loss: 2.0549 - val_acc: 0.7350\n",
      "Epoch 36/100\n",
      "125/125 [==============================] - 31s 249ms/step - loss: 0.7254 - acc: 0.8669 - val_loss: 2.0689 - val_acc: 0.7322\n",
      "Epoch 37/100\n",
      "125/125 [==============================] - 30s 242ms/step - loss: 0.7285 - acc: 0.8682 - val_loss: 2.0763 - val_acc: 0.7330\n",
      "Epoch 38/100\n",
      "125/125 [==============================] - 26s 206ms/step - loss: 0.7152 - acc: 0.8700 - val_loss: 2.0753 - val_acc: 0.7343\n",
      "Epoch 39/100\n",
      "125/125 [==============================] - 27s 213ms/step - loss: 0.7036 - acc: 0.8737 - val_loss: 2.0951 - val_acc: 0.7346\n",
      "Epoch 40/100\n",
      "125/125 [==============================] - 18s 140ms/step - loss: 0.6954 - acc: 0.8715 - val_loss: 2.0684 - val_acc: 0.7365\n",
      "Epoch 41/100\n",
      "125/125 [==============================] - 18s 141ms/step - loss: 0.6954 - acc: 0.8731 - val_loss: 2.0902 - val_acc: 0.7359\n",
      "Epoch 42/100\n",
      "125/125 [==============================] - 17s 139ms/step - loss: 0.6864 - acc: 0.8732 - val_loss: 2.0899 - val_acc: 0.7370\n",
      "Epoch 43/100\n",
      "125/125 [==============================] - 19s 152ms/step - loss: 0.6729 - acc: 0.8759 - val_loss: 2.0955 - val_acc: 0.7358\n",
      "Epoch 44/100\n",
      "125/125 [==============================] - 22s 179ms/step - loss: 0.6649 - acc: 0.8798 - val_loss: 2.1018 - val_acc: 0.7345\n",
      "Epoch 45/100\n",
      "125/125 [==============================] - 24s 189ms/step - loss: 0.6646 - acc: 0.8784 - val_loss: 2.1009 - val_acc: 0.7357\n",
      "Epoch 46/100\n",
      "125/125 [==============================] - 23s 184ms/step - loss: 0.6514 - acc: 0.8814 - val_loss: 2.1113 - val_acc: 0.7343\n",
      "Epoch 47/100\n",
      "125/125 [==============================] - 24s 189ms/step - loss: 0.6452 - acc: 0.8815 - val_loss: 2.1125 - val_acc: 0.7332\n",
      "Epoch 48/100\n",
      "125/125 [==============================] - 24s 190ms/step - loss: 0.6352 - acc: 0.8825 - val_loss: 2.1192 - val_acc: 0.7354\n",
      "Epoch 49/100\n",
      "125/125 [==============================] - 23s 181ms/step - loss: 0.6347 - acc: 0.8839 - val_loss: 2.1288 - val_acc: 0.7360\n",
      "Epoch 50/100\n",
      "125/125 [==============================] - 28s 229ms/step - loss: 0.6276 - acc: 0.8858 - val_loss: 2.1301 - val_acc: 0.7373\n",
      "Epoch 51/100\n",
      "125/125 [==============================] - 25s 199ms/step - loss: 0.6196 - acc: 0.8860 - val_loss: 2.1474 - val_acc: 0.7353\n",
      "Epoch 52/100\n",
      "125/125 [==============================] - 24s 192ms/step - loss: 0.6182 - acc: 0.8868 - val_loss: 2.1540 - val_acc: 0.7359\n",
      "Epoch 53/100\n",
      "125/125 [==============================] - 23s 182ms/step - loss: 0.6117 - acc: 0.8888 - val_loss: 2.1592 - val_acc: 0.7357\n",
      "Epoch 54/100\n",
      "125/125 [==============================] - 21s 169ms/step - loss: 0.6072 - acc: 0.8884 - val_loss: 2.1732 - val_acc: 0.7373\n",
      "Epoch 55/100\n",
      "125/125 [==============================] - 20s 159ms/step - loss: 0.5923 - acc: 0.8903 - val_loss: 2.1825 - val_acc: 0.7338\n",
      "Epoch 56/100\n",
      "125/125 [==============================] - 23s 181ms/step - loss: 0.5889 - acc: 0.8903 - val_loss: 2.1842 - val_acc: 0.7349\n",
      "Epoch 57/100\n",
      "125/125 [==============================] - 25s 202ms/step - loss: 0.5826 - acc: 0.8909 - val_loss: 2.1895 - val_acc: 0.7346\n",
      "Epoch 58/100\n",
      "125/125 [==============================] - 19s 152ms/step - loss: 0.5844 - acc: 0.8915 - val_loss: 2.1770 - val_acc: 0.7357\n",
      "Epoch 59/100\n",
      "125/125 [==============================] - 19s 151ms/step - loss: 0.5809 - acc: 0.8920 - val_loss: 2.2055 - val_acc: 0.7348\n",
      "Epoch 60/100\n",
      "125/125 [==============================] - 19s 149ms/step - loss: 0.5703 - acc: 0.8945 - val_loss: 2.2046 - val_acc: 0.7349\n",
      "Epoch 61/100\n",
      "125/125 [==============================] - 18s 147ms/step - loss: 0.5725 - acc: 0.8941 - val_loss: 2.1946 - val_acc: 0.7344\n",
      "Epoch 62/100\n",
      "125/125 [==============================] - 21s 171ms/step - loss: 0.5703 - acc: 0.8957 - val_loss: 2.2069 - val_acc: 0.7331\n",
      "Epoch 63/100\n",
      "125/125 [==============================] - 25s 200ms/step - loss: 0.5663 - acc: 0.8960 - val_loss: 2.2033 - val_acc: 0.7345\n",
      "Epoch 64/100\n",
      "125/125 [==============================] - 27s 214ms/step - loss: 0.5628 - acc: 0.8956 - val_loss: 2.2235 - val_acc: 0.7341\n",
      "Epoch 65/100\n",
      "125/125 [==============================] - 27s 213ms/step - loss: 0.5664 - acc: 0.8968 - val_loss: 2.2119 - val_acc: 0.7333\n",
      "Epoch 66/100\n",
      "125/125 [==============================] - 26s 211ms/step - loss: 0.5629 - acc: 0.8972 - val_loss: 2.2142 - val_acc: 0.7336\n",
      "Epoch 67/100\n",
      "125/125 [==============================] - 27s 214ms/step - loss: 0.5552 - acc: 0.8986 - val_loss: 2.2080 - val_acc: 0.7321\n",
      "Epoch 68/100\n",
      "125/125 [==============================] - 27s 218ms/step - loss: 0.5590 - acc: 0.8980 - val_loss: 2.2143 - val_acc: 0.7334\n",
      "Epoch 69/100\n",
      "125/125 [==============================] - 27s 212ms/step - loss: 0.5615 - acc: 0.8984 - val_loss: 2.2136 - val_acc: 0.7334\n",
      "Epoch 70/100\n",
      "125/125 [==============================] - 25s 202ms/step - loss: 0.5514 - acc: 0.8994 - val_loss: 2.2041 - val_acc: 0.7339\n",
      "Epoch 71/100\n",
      "125/125 [==============================] - 25s 201ms/step - loss: 0.5455 - acc: 0.9008 - val_loss: 2.2194 - val_acc: 0.7337\n",
      "Epoch 72/100\n",
      "125/125 [==============================] - 25s 198ms/step - loss: 0.5493 - acc: 0.8998 - val_loss: 2.2226 - val_acc: 0.7341\n",
      "Epoch 73/100\n",
      "125/125 [==============================] - 25s 199ms/step - loss: 0.5489 - acc: 0.8997 - val_loss: 2.2230 - val_acc: 0.7342\n",
      "Epoch 74/100\n",
      "125/125 [==============================] - 25s 200ms/step - loss: 0.5481 - acc: 0.8997 - val_loss: 2.2172 - val_acc: 0.7346\n",
      "Epoch 75/100\n",
      "125/125 [==============================] - 25s 201ms/step - loss: 0.5455 - acc: 0.9006 - val_loss: 2.2172 - val_acc: 0.7347\n",
      "Epoch 76/100\n",
      "125/125 [==============================] - 25s 202ms/step - loss: 0.5405 - acc: 0.9013 - val_loss: 2.2149 - val_acc: 0.7338\n",
      "Epoch 77/100\n",
      "125/125 [==============================] - 25s 202ms/step - loss: 0.5428 - acc: 0.9000 - val_loss: 2.2238 - val_acc: 0.7356\n",
      "Epoch 78/100\n",
      "125/125 [==============================] - 25s 200ms/step - loss: 0.5289 - acc: 0.9029 - val_loss: 2.2272 - val_acc: 0.7354\n",
      "Epoch 79/100\n",
      "125/125 [==============================] - 25s 200ms/step - loss: 0.5323 - acc: 0.9023 - val_loss: 2.2326 - val_acc: 0.7337\n",
      "Epoch 80/100\n",
      "125/125 [==============================] - 26s 212ms/step - loss: 0.5305 - acc: 0.9033 - val_loss: 2.2295 - val_acc: 0.7341\n",
      "Epoch 81/100\n",
      "125/125 [==============================] - 24s 195ms/step - loss: 0.5299 - acc: 0.9043 - val_loss: 2.2336 - val_acc: 0.7331\n",
      "Epoch 82/100\n",
      "125/125 [==============================] - 25s 197ms/step - loss: 0.5242 - acc: 0.9042 - val_loss: 2.2225 - val_acc: 0.7347\n",
      "Epoch 83/100\n",
      "125/125 [==============================] - 25s 198ms/step - loss: 0.5174 - acc: 0.9062 - val_loss: 2.2267 - val_acc: 0.7323\n",
      "Epoch 84/100\n",
      "125/125 [==============================] - 25s 200ms/step - loss: 0.5187 - acc: 0.9055 - val_loss: 2.2387 - val_acc: 0.7332\n",
      "Epoch 85/100\n",
      "125/125 [==============================] - 25s 197ms/step - loss: 0.5064 - acc: 0.9085 - val_loss: 2.2334 - val_acc: 0.7349\n",
      "Epoch 86/100\n",
      "125/125 [==============================] - 25s 200ms/step - loss: 0.5181 - acc: 0.9041 - val_loss: 2.2356 - val_acc: 0.7342\n",
      "Epoch 87/100\n",
      "125/125 [==============================] - 25s 201ms/step - loss: 0.5102 - acc: 0.9069 - val_loss: 2.2239 - val_acc: 0.7348\n",
      "Epoch 88/100\n",
      "125/125 [==============================] - 25s 201ms/step - loss: 0.5113 - acc: 0.9051 - val_loss: 2.2410 - val_acc: 0.7321\n",
      "Epoch 89/100\n",
      "125/125 [==============================] - 25s 201ms/step - loss: 0.5128 - acc: 0.9062 - val_loss: 2.2363 - val_acc: 0.7337\n",
      "Epoch 90/100\n",
      "125/125 [==============================] - 25s 199ms/step - loss: 0.5023 - acc: 0.9076 - val_loss: 2.2363 - val_acc: 0.7343\n",
      "Epoch 91/100\n",
      "125/125 [==============================] - 25s 201ms/step - loss: 0.4986 - acc: 0.9087 - val_loss: 2.2457 - val_acc: 0.7332\n",
      "Epoch 92/100\n",
      "125/125 [==============================] - 26s 209ms/step - loss: 0.4971 - acc: 0.9077 - val_loss: 2.2457 - val_acc: 0.7331\n",
      "Epoch 93/100\n",
      "125/125 [==============================] - 25s 199ms/step - loss: 0.5035 - acc: 0.9078 - val_loss: 2.2461 - val_acc: 0.7336\n",
      "Epoch 94/100\n",
      "125/125 [==============================] - 25s 198ms/step - loss: 0.4969 - acc: 0.9080 - val_loss: 2.2526 - val_acc: 0.7338\n",
      "Epoch 95/100\n",
      "125/125 [==============================] - 26s 206ms/step - loss: 0.5006 - acc: 0.9088 - val_loss: 2.2646 - val_acc: 0.7331\n",
      "Epoch 96/100\n",
      "125/125 [==============================] - 34s 276ms/step - loss: 0.4922 - acc: 0.9096 - val_loss: 2.2591 - val_acc: 0.7347\n",
      "Epoch 97/100\n",
      "125/125 [==============================] - 35s 283ms/step - loss: 0.4906 - acc: 0.9106 - val_loss: 2.2650 - val_acc: 0.7316\n",
      "Epoch 98/100\n",
      "125/125 [==============================] - 30s 237ms/step - loss: 0.4905 - acc: 0.9098 - val_loss: 2.2611 - val_acc: 0.7333\n",
      "Epoch 99/100\n",
      "125/125 [==============================] - 23s 180ms/step - loss: 0.4903 - acc: 0.9114 - val_loss: 2.2689 - val_acc: 0.7329\n",
      "Epoch 100/100\n",
      "125/125 [==============================] - 24s 196ms/step - loss: 0.4864 - acc: 0.9103 - val_loss: 2.2800 - val_acc: 0.7334\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f6f1f987a00>"
      ]
     },
     "metadata": {},
     "execution_count": 32
    }
   ],
   "source": [
    "model.fit(\n",
    "    [encoder_inputs, decoder_inputs],\n",
    "    decoder_targets_one_hot,\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    validation_split=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('s2s.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREDICTION MODEL\n",
    "encoder_model = Model(encoder_inputs_placeholder, encoder_states)\n",
    "\n",
    "decoder_state_input_h = Input(shape=(LATENT_DIM,))\n",
    "decoder_state_input_c = Input(shape=(LATENT_DIM,))\n",
    "decoder_state_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "decoder_inputs_single = Input(shape=(1,))\n",
    "decoder_inputs_single_x = decoder_embedding(decoder_inputs_single)\n",
    "\n",
    "decoder_outputs, h, c = decoder_LSTM(\n",
    "    decoder_inputs_single_x,\n",
    "    initial_state= decoder_state_inputs\n",
    ")\n",
    "decoder_states = [h,c]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs_single] + decoder_state_inputs,\n",
    "    [decoder_outputs] + decoder_states\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx2word_eg = {v:k for k,v in word2idx_input.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx2word_trans = {v:k for k,v in word2idx_output.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    target_seq = np.zeros((1,1))\n",
    "\n",
    "    target_seq[0, 0] = word2idx_output['<sos>']\n",
    "\n",
    "    eos = word2idx_output['<eos>']\n",
    "\n",
    "    output_sentence = []\n",
    "\n",
    "    for _ in range(max_len_output):\n",
    "        output_tokens, h, c = decoder_model.predict(\n",
    "            [target_seq] + states_value\n",
    "        )\n",
    "\n",
    "        idx = np.argmax(output_tokens[0,0,:])\n",
    "\n",
    "        if eos == idx:\n",
    "            break\n",
    "\n",
    "        word = ''\n",
    "        if idx > 0:\n",
    "            word = idx2word_trans[idx]\n",
    "            output_sentence.append(word)\n",
    "\n",
    "        target_seq[0,0] = idx\n",
    "\n",
    "        states_value = [h,c]\n",
    "\n",
    "    return ' '.join(output_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "-\n",
      "Input: That's stupid.\n",
      "Translation: eso es estúpido.\n",
      "-\n",
      "Input: Tom isn't dumb.\n",
      "Translation: tom no es estúpido.\n",
      "-\n",
      "Input: I fell.\n",
      "Translation: me caí.\n",
      "-\n",
      "Input: Tom was happy.\n",
      "Translation: tom estaba feliz.\n",
      "-\n",
      "Input: Can I keep it?\n",
      "Translation: ¿puedo tan prestado?\n",
      "-\n",
      "Input: Stop fighting!\n",
      "Translation: no te muevas.\n",
      "-\n",
      "Input: Stay together.\n",
      "Translation: no os muevas.\n",
      "-\n",
      "Input: You are mad.\n",
      "Translation: estás loca.\n",
      "-\n",
      "Input: My name's Tom.\n",
      "Translation: mi nombre es tom.\n",
      "-\n",
      "Input: It's magic.\n",
      "Translation: es magia.\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    i = np.random.choice(len(input_texts))\n",
    "    input_seq = encoder_inputs[i:i+1]\n",
    "    translation = decode_sequence(input_seq)\n",
    "    print('-')\n",
    "    print(f'Input: {input_texts[i]}')\n",
    "    print(f'Translation: {translation}')\n",
    "\n",
    "    ans = input(\"Continue? [Y/n]\")\n",
    "    if ans and ans.lower().startswith('n'):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}