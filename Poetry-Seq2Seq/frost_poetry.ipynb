{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python385jvsc74a57bd0916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1",
   "display_name": "Python 3.8.5 64-bit"
  },
  "metadata": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import LSTM, Embedding, Input, Dense\n",
    "from keras.models import Model\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 20000\n",
    "EMEBDDING_DIM = 50\n",
    "MAX_SEQUENCE_LENGTH = 100\n",
    "VALIDATION_SPLIT = 0.2\n",
    "EPOCHS = 1000\n",
    "BATCH_SIZE = 128\n",
    "LATENT_DIM = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_texts = []\n",
    "target_texts = []\n",
    "for line in open('robert_frost.txt'):\n",
    "    line = line.rstrip()\n",
    "    if not line:\n",
    "        continue\n",
    "\n",
    "    input_line = '<sos> ' + line\n",
    "    target_line = line + ' <eos>'\n",
    "\n",
    "    input_texts.append(input_line)\n",
    "    target_texts.append(target_line)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_lines = input_texts + target_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=VOCAB_SIZE, filters='')\n",
    "tokenizer.fit_on_texts(all_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sequences = tokenizer.texts_to_sequences(input_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_sequences = tokenizer.texts_to_sequences(target_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "1436"
      ]
     },
     "metadata": {},
     "execution_count": 67
    }
   ],
   "source": [
    "len(target_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sequence_length_from_data = max(len(s) for s in input_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "metadata": {},
     "execution_count": 69
    }
   ],
   "source": [
    "max_sequence_length_from_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2idx = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sequence_length = min(max_sequence_length_from_data, MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "metadata": {},
     "execution_count": 72
    }
   ],
   "source": [
    "max_sequence_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sequences = pad_sequences(input_sequences, maxlen=max_sequence_length, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_sequences = pad_sequences(target_sequences, maxlen=max_sequence_length, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec = {}\n",
    "\n",
    "with open('glove.6B.50d.txt') as f:\n",
    "    for lines in f:\n",
    "        values = lines.split()\n",
    "        word = values[0]\n",
    "        vec = np.asarray(values[1:], dtype='float32')\n",
    "        word2vec[word] = vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "400000"
      ]
     },
     "metadata": {},
     "execution_count": 76
    }
   ],
   "source": [
    "len(word2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_words = min(VOCAB_SIZE, len(word2idx) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "3057"
      ]
     },
     "metadata": {},
     "execution_count": 78
    }
   ],
   "source": [
    "num_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((num_words, EMEBDDING_DIM))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word, i in word2idx.items():\n",
    "    if i < VOCAB_SIZE:\n",
    "        embedding_vector = word2vec.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_targets = np.zeros((len(input_sequences), max_sequence_length, num_words))\n",
    "for i, target_sequence in enumerate(target_sequences):\n",
    "    for t, word in enumerate(target_sequence):\n",
    "        if word > 0:\n",
    "            one_hot_targets[i, t, word] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(1436, 12, 3057)\n(1436, 12)\n"
     ]
    }
   ],
   "source": [
    "print(one_hot_targets.shape)\n",
    "print(target_sequences.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "metadata": {},
     "execution_count": 83
    }
   ],
   "source": [
    "one_hot_targets[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([  5, 541,   6,  65,  31, 934, 141,   2,   0,   0,   0,   0],\n",
       "      dtype=int32)"
      ]
     },
     "metadata": {},
     "execution_count": 84
    }
   ],
   "source": [
    "target_sequences[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = Embedding(\n",
    "    num_words,\n",
    "    EMEBDDING_DIM,\n",
    "    weights=[embedding_matrix]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ = Input(shape=(max_sequence_length,))\n",
    "initial_h = Input(shape=(LATENT_DIM,))\n",
    "intial_c = Input(shape=(LATENT_DIM,))\n",
    "x = embedding_layer(input_)\n",
    "lstm = LSTM(LATENT_DIM, return_sequences=True, return_state=True)\n",
    "x,_,_ = lstm(x, initial_state=[initial_h, intial_c])\n",
    "dense = Dense(num_words, activation='softmax')\n",
    "output = dense(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model([input_, initial_h, intial_c], output)\n",
    "model.compile(\n",
    "    loss='categorical_crossentropy',\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "ss: 6.0026 - val_accuracy: 0.0839\n",
      "Epoch 862/1000\n",
      "9/9 [==============================] - 1s 100ms/step - loss: 2.0972 - accuracy: 0.2868 - val_loss: 6.0078 - val_accuracy: 0.0839\n",
      "Epoch 863/1000\n",
      "9/9 [==============================] - 1s 92ms/step - loss: 2.1078 - accuracy: 0.2870 - val_loss: 6.0080 - val_accuracy: 0.0845\n",
      "Epoch 864/1000\n",
      "9/9 [==============================] - 1s 83ms/step - loss: 2.0852 - accuracy: 0.2905 - val_loss: 6.0127 - val_accuracy: 0.0836\n",
      "Epoch 865/1000\n",
      "9/9 [==============================] - 1s 89ms/step - loss: 2.1576 - accuracy: 0.2867 - val_loss: 6.0115 - val_accuracy: 0.0845\n",
      "Epoch 866/1000\n",
      "9/9 [==============================] - 1s 111ms/step - loss: 2.1178 - accuracy: 0.2875 - val_loss: 6.0168 - val_accuracy: 0.0836\n",
      "Epoch 867/1000\n",
      "9/9 [==============================] - 1s 84ms/step - loss: 2.0622 - accuracy: 0.2931 - val_loss: 6.0191 - val_accuracy: 0.0845\n",
      "Epoch 868/1000\n",
      "9/9 [==============================] - 1s 102ms/step - loss: 2.0721 - accuracy: 0.2842 - val_loss: 6.0217 - val_accuracy: 0.0833\n",
      "Epoch 869/1000\n",
      "9/9 [==============================] - 1s 95ms/step - loss: 2.1145 - accuracy: 0.2931 - val_loss: 6.0219 - val_accuracy: 0.0842\n",
      "Epoch 870/1000\n",
      "9/9 [==============================] - 1s 86ms/step - loss: 2.0870 - accuracy: 0.2912 - val_loss: 6.0244 - val_accuracy: 0.0836\n",
      "Epoch 871/1000\n",
      "9/9 [==============================] - 1s 98ms/step - loss: 2.1007 - accuracy: 0.2871 - val_loss: 6.0279 - val_accuracy: 0.0836\n",
      "Epoch 872/1000\n",
      "9/9 [==============================] - 1s 84ms/step - loss: 2.0512 - accuracy: 0.2927 - val_loss: 6.0287 - val_accuracy: 0.0839\n",
      "Epoch 873/1000\n",
      "9/9 [==============================] - 1s 98ms/step - loss: 2.0818 - accuracy: 0.2890 - val_loss: 6.0351 - val_accuracy: 0.0830\n",
      "Epoch 874/1000\n",
      "9/9 [==============================] - 1s 127ms/step - loss: 2.0436 - accuracy: 0.2910 - val_loss: 6.0328 - val_accuracy: 0.0839\n",
      "Epoch 875/1000\n",
      "9/9 [==============================] - 1s 158ms/step - loss: 2.0708 - accuracy: 0.2971 - val_loss: 6.0367 - val_accuracy: 0.0836\n",
      "Epoch 876/1000\n",
      "9/9 [==============================] - 1s 87ms/step - loss: 2.1037 - accuracy: 0.2862 - val_loss: 6.0363 - val_accuracy: 0.0836\n",
      "Epoch 877/1000\n",
      "9/9 [==============================] - 1s 127ms/step - loss: 2.1302 - accuracy: 0.2893 - val_loss: 6.0413 - val_accuracy: 0.0833\n",
      "Epoch 878/1000\n",
      "9/9 [==============================] - 1s 121ms/step - loss: 2.0934 - accuracy: 0.2857 - val_loss: 6.0411 - val_accuracy: 0.0830\n",
      "Epoch 879/1000\n",
      "9/9 [==============================] - 1s 141ms/step - loss: 2.0748 - accuracy: 0.2895 - val_loss: 6.0451 - val_accuracy: 0.0836\n",
      "Epoch 880/1000\n",
      "9/9 [==============================] - 1s 116ms/step - loss: 2.0740 - accuracy: 0.2946 - val_loss: 6.0477 - val_accuracy: 0.0836\n",
      "Epoch 881/1000\n",
      "9/9 [==============================] - 1s 120ms/step - loss: 2.0772 - accuracy: 0.2956 - val_loss: 6.0532 - val_accuracy: 0.0830\n",
      "Epoch 882/1000\n",
      "9/9 [==============================] - 1s 125ms/step - loss: 2.0833 - accuracy: 0.2910 - val_loss: 6.0492 - val_accuracy: 0.0830\n",
      "Epoch 883/1000\n",
      "9/9 [==============================] - 1s 131ms/step - loss: 2.0741 - accuracy: 0.2976 - val_loss: 6.0528 - val_accuracy: 0.0833\n",
      "Epoch 884/1000\n",
      "9/9 [==============================] - 1s 138ms/step - loss: 2.0880 - accuracy: 0.2896 - val_loss: 6.0563 - val_accuracy: 0.0833\n",
      "Epoch 885/1000\n",
      "9/9 [==============================] - 1s 130ms/step - loss: 2.0726 - accuracy: 0.2918 - val_loss: 6.0576 - val_accuracy: 0.0839\n",
      "Epoch 886/1000\n",
      "9/9 [==============================] - 1s 146ms/step - loss: 2.0707 - accuracy: 0.2913 - val_loss: 6.0609 - val_accuracy: 0.0833\n",
      "Epoch 887/1000\n",
      "9/9 [==============================] - 1s 138ms/step - loss: 2.0821 - accuracy: 0.2978 - val_loss: 6.0601 - val_accuracy: 0.0839\n",
      "Epoch 888/1000\n",
      "9/9 [==============================] - 1s 98ms/step - loss: 2.0866 - accuracy: 0.2925 - val_loss: 6.0680 - val_accuracy: 0.0830\n",
      "Epoch 889/1000\n",
      "9/9 [==============================] - 1s 100ms/step - loss: 2.0874 - accuracy: 0.2947 - val_loss: 6.0675 - val_accuracy: 0.0842\n",
      "Epoch 890/1000\n",
      "9/9 [==============================] - 1s 86ms/step - loss: 2.0686 - accuracy: 0.2936 - val_loss: 6.0686 - val_accuracy: 0.0839\n",
      "Epoch 891/1000\n",
      "9/9 [==============================] - 1s 113ms/step - loss: 2.0914 - accuracy: 0.2900 - val_loss: 6.0690 - val_accuracy: 0.0839\n",
      "Epoch 892/1000\n",
      "9/9 [==============================] - 1s 95ms/step - loss: 2.0754 - accuracy: 0.2939 - val_loss: 6.0759 - val_accuracy: 0.0833\n",
      "Epoch 893/1000\n",
      "9/9 [==============================] - 1s 111ms/step - loss: 2.0564 - accuracy: 0.2922 - val_loss: 6.0737 - val_accuracy: 0.0836\n",
      "Epoch 894/1000\n",
      "9/9 [==============================] - 1s 95ms/step - loss: 2.0529 - accuracy: 0.2963 - val_loss: 6.0760 - val_accuracy: 0.0839\n",
      "Epoch 895/1000\n",
      "9/9 [==============================] - 1s 96ms/step - loss: 2.0707 - accuracy: 0.2949 - val_loss: 6.0802 - val_accuracy: 0.0830\n",
      "Epoch 896/1000\n",
      "9/9 [==============================] - 1s 93ms/step - loss: 2.0714 - accuracy: 0.2928 - val_loss: 6.0827 - val_accuracy: 0.0833\n",
      "Epoch 897/1000\n",
      "9/9 [==============================] - 1s 92ms/step - loss: 2.0647 - accuracy: 0.2959 - val_loss: 6.0851 - val_accuracy: 0.0833\n",
      "Epoch 898/1000\n",
      "9/9 [==============================] - 1s 96ms/step - loss: 2.0433 - accuracy: 0.2953 - val_loss: 6.0838 - val_accuracy: 0.0839\n",
      "Epoch 899/1000\n",
      "9/9 [==============================] - 1s 86ms/step - loss: 2.0447 - accuracy: 0.2972 - val_loss: 6.0928 - val_accuracy: 0.0828\n",
      "Epoch 900/1000\n",
      "9/9 [==============================] - 1s 97ms/step - loss: 2.0755 - accuracy: 0.2948 - val_loss: 6.0893 - val_accuracy: 0.0842\n",
      "Epoch 901/1000\n",
      "9/9 [==============================] - 1s 104ms/step - loss: 2.0657 - accuracy: 0.2945 - val_loss: 6.0951 - val_accuracy: 0.0833\n",
      "Epoch 902/1000\n",
      "9/9 [==============================] - 1s 111ms/step - loss: 2.0635 - accuracy: 0.2922 - val_loss: 6.0966 - val_accuracy: 0.0842\n",
      "Epoch 903/1000\n",
      "9/9 [==============================] - 1s 114ms/step - loss: 2.0643 - accuracy: 0.2956 - val_loss: 6.0993 - val_accuracy: 0.0833\n",
      "Epoch 904/1000\n",
      "9/9 [==============================] - 1s 107ms/step - loss: 2.0369 - accuracy: 0.2938 - val_loss: 6.1043 - val_accuracy: 0.0833\n",
      "Epoch 905/1000\n",
      "9/9 [==============================] - 1s 107ms/step - loss: 2.0542 - accuracy: 0.2962 - val_loss: 6.1044 - val_accuracy: 0.0836\n",
      "Epoch 906/1000\n",
      "9/9 [==============================] - 1s 115ms/step - loss: 2.0827 - accuracy: 0.2918 - val_loss: 6.1054 - val_accuracy: 0.0830\n",
      "Epoch 907/1000\n",
      "9/9 [==============================] - 1s 115ms/step - loss: 2.0341 - accuracy: 0.2960 - val_loss: 6.1098 - val_accuracy: 0.0836\n",
      "Epoch 908/1000\n",
      "9/9 [==============================] - 1s 105ms/step - loss: 2.0516 - accuracy: 0.3000 - val_loss: 6.1112 - val_accuracy: 0.0830\n",
      "Epoch 909/1000\n",
      "9/9 [==============================] - 1s 108ms/step - loss: 2.0239 - accuracy: 0.3007 - val_loss: 6.1170 - val_accuracy: 0.0822\n",
      "Epoch 910/1000\n",
      "9/9 [==============================] - 1s 114ms/step - loss: 2.0495 - accuracy: 0.2938 - val_loss: 6.1144 - val_accuracy: 0.0842\n",
      "Epoch 911/1000\n",
      "9/9 [==============================] - 1s 97ms/step - loss: 2.0488 - accuracy: 0.2902 - val_loss: 6.1185 - val_accuracy: 0.0830\n",
      "Epoch 912/1000\n",
      "9/9 [==============================] - 1s 117ms/step - loss: 2.0284 - accuracy: 0.2964 - val_loss: 6.1228 - val_accuracy: 0.0833\n",
      "Epoch 913/1000\n",
      "9/9 [==============================] - 1s 143ms/step - loss: 2.0763 - accuracy: 0.2922 - val_loss: 6.1265 - val_accuracy: 0.0839\n",
      "Epoch 914/1000\n",
      "9/9 [==============================] - 1s 127ms/step - loss: 2.0364 - accuracy: 0.2953 - val_loss: 6.1270 - val_accuracy: 0.0836\n",
      "Epoch 915/1000\n",
      "9/9 [==============================] - 1s 162ms/step - loss: 2.0363 - accuracy: 0.2954 - val_loss: 6.1304 - val_accuracy: 0.0836\n",
      "Epoch 916/1000\n",
      "9/9 [==============================] - 1s 141ms/step - loss: 2.0698 - accuracy: 0.2960 - val_loss: 6.1292 - val_accuracy: 0.0833\n",
      "Epoch 917/1000\n",
      "9/9 [==============================] - 1s 128ms/step - loss: 2.0630 - accuracy: 0.2977 - val_loss: 6.1353 - val_accuracy: 0.0830\n",
      "Epoch 918/1000\n",
      "9/9 [==============================] - 1s 123ms/step - loss: 2.0418 - accuracy: 0.2961 - val_loss: 6.1349 - val_accuracy: 0.0836\n",
      "Epoch 919/1000\n",
      "9/9 [==============================] - 1s 96ms/step - loss: 2.0366 - accuracy: 0.2996 - val_loss: 6.1383 - val_accuracy: 0.0830\n",
      "Epoch 920/1000\n",
      "9/9 [==============================] - 1s 136ms/step - loss: 2.0552 - accuracy: 0.2956 - val_loss: 6.1408 - val_accuracy: 0.0833\n",
      "Epoch 921/1000\n",
      "9/9 [==============================] - 1s 114ms/step - loss: 2.0503 - accuracy: 0.2944 - val_loss: 6.1422 - val_accuracy: 0.0833\n",
      "Epoch 922/1000\n",
      "9/9 [==============================] - 1s 103ms/step - loss: 2.0594 - accuracy: 0.2927 - val_loss: 6.1469 - val_accuracy: 0.0828\n",
      "Epoch 923/1000\n",
      "9/9 [==============================] - 1s 111ms/step - loss: 2.0602 - accuracy: 0.2992 - val_loss: 6.1489 - val_accuracy: 0.0833\n",
      "Epoch 924/1000\n",
      "9/9 [==============================] - 1s 101ms/step - loss: 2.0689 - accuracy: 0.3009 - val_loss: 6.1511 - val_accuracy: 0.0825\n",
      "Epoch 925/1000\n",
      "9/9 [==============================] - 1s 124ms/step - loss: 2.0234 - accuracy: 0.2975 - val_loss: 6.1509 - val_accuracy: 0.0833\n",
      "Epoch 926/1000\n",
      "9/9 [==============================] - 1s 128ms/step - loss: 2.0220 - accuracy: 0.2996 - val_loss: 6.1586 - val_accuracy: 0.0830\n",
      "Epoch 927/1000\n",
      "9/9 [==============================] - 1s 110ms/step - loss: 2.0401 - accuracy: 0.2973 - val_loss: 6.1561 - val_accuracy: 0.0836\n",
      "Epoch 928/1000\n",
      "9/9 [==============================] - 1s 126ms/step - loss: 2.0401 - accuracy: 0.3037 - val_loss: 6.1591 - val_accuracy: 0.0833\n",
      "Epoch 929/1000\n",
      "9/9 [==============================] - 1s 95ms/step - loss: 1.9827 - accuracy: 0.2991 - val_loss: 6.1611 - val_accuracy: 0.0833\n",
      "Epoch 930/1000\n",
      "9/9 [==============================] - 1s 100ms/step - loss: 2.0395 - accuracy: 0.2996 - val_loss: 6.1650 - val_accuracy: 0.0828\n",
      "Epoch 931/1000\n",
      "9/9 [==============================] - 1s 110ms/step - loss: 2.0163 - accuracy: 0.2970 - val_loss: 6.1671 - val_accuracy: 0.0830\n",
      "Epoch 932/1000\n",
      "9/9 [==============================] - 1s 93ms/step - loss: 2.0413 - accuracy: 0.2976 - val_loss: 6.1686 - val_accuracy: 0.0830\n",
      "Epoch 933/1000\n",
      "9/9 [==============================] - 1s 82ms/step - loss: 2.0182 - accuracy: 0.3026 - val_loss: 6.1718 - val_accuracy: 0.0833\n",
      "Epoch 934/1000\n",
      "9/9 [==============================] - 1s 103ms/step - loss: 2.0173 - accuracy: 0.2999 - val_loss: 6.1731 - val_accuracy: 0.0830\n",
      "Epoch 935/1000\n",
      "9/9 [==============================] - 1s 89ms/step - loss: 2.0239 - accuracy: 0.2999 - val_loss: 6.1786 - val_accuracy: 0.0819\n",
      "Epoch 936/1000\n",
      "9/9 [==============================] - 1s 91ms/step - loss: 2.0538 - accuracy: 0.2998 - val_loss: 6.1795 - val_accuracy: 0.0830\n",
      "Epoch 937/1000\n",
      "9/9 [==============================] - 1s 100ms/step - loss: 1.9973 - accuracy: 0.3005 - val_loss: 6.1823 - val_accuracy: 0.0825\n",
      "Epoch 938/1000\n",
      "9/9 [==============================] - 1s 96ms/step - loss: 2.0272 - accuracy: 0.2966 - val_loss: 6.1826 - val_accuracy: 0.0836\n",
      "Epoch 939/1000\n",
      "9/9 [==============================] - 1s 110ms/step - loss: 2.0059 - accuracy: 0.3052 - val_loss: 6.1924 - val_accuracy: 0.0828\n",
      "Epoch 940/1000\n",
      "9/9 [==============================] - 1s 88ms/step - loss: 2.0398 - accuracy: 0.3024 - val_loss: 6.1925 - val_accuracy: 0.0825\n",
      "Epoch 941/1000\n",
      "9/9 [==============================] - 1s 96ms/step - loss: 2.0319 - accuracy: 0.3011 - val_loss: 6.1942 - val_accuracy: 0.0830\n",
      "Epoch 942/1000\n",
      "9/9 [==============================] - 1s 92ms/step - loss: 2.0097 - accuracy: 0.3041 - val_loss: 6.1972 - val_accuracy: 0.0828\n",
      "Epoch 943/1000\n",
      "9/9 [==============================] - 1s 95ms/step - loss: 2.0145 - accuracy: 0.3050 - val_loss: 6.1996 - val_accuracy: 0.0822\n",
      "Epoch 944/1000\n",
      "9/9 [==============================] - 1s 84ms/step - loss: 2.0217 - accuracy: 0.3024 - val_loss: 6.2019 - val_accuracy: 0.0825\n",
      "Epoch 945/1000\n",
      "9/9 [==============================] - 1s 105ms/step - loss: 2.0060 - accuracy: 0.3009 - val_loss: 6.2041 - val_accuracy: 0.0833\n",
      "Epoch 946/1000\n",
      "9/9 [==============================] - 1s 100ms/step - loss: 2.0203 - accuracy: 0.3031 - val_loss: 6.2061 - val_accuracy: 0.0825\n",
      "Epoch 947/1000\n",
      "9/9 [==============================] - 1s 117ms/step - loss: 1.9998 - accuracy: 0.3019 - val_loss: 6.2075 - val_accuracy: 0.0828\n",
      "Epoch 948/1000\n",
      "9/9 [==============================] - 1s 82ms/step - loss: 1.9976 - accuracy: 0.2979 - val_loss: 6.2125 - val_accuracy: 0.0822\n",
      "Epoch 949/1000\n",
      "9/9 [==============================] - 1s 109ms/step - loss: 1.9903 - accuracy: 0.3010 - val_loss: 6.2153 - val_accuracy: 0.0822\n",
      "Epoch 950/1000\n",
      "9/9 [==============================] - 1s 97ms/step - loss: 2.0174 - accuracy: 0.2981 - val_loss: 6.2169 - val_accuracy: 0.0822\n",
      "Epoch 951/1000\n",
      "9/9 [==============================] - 1s 94ms/step - loss: 1.9925 - accuracy: 0.3063 - val_loss: 6.2212 - val_accuracy: 0.0819\n",
      "Epoch 952/1000\n",
      "9/9 [==============================] - 1s 111ms/step - loss: 2.0252 - accuracy: 0.2996 - val_loss: 6.2215 - val_accuracy: 0.0822\n",
      "Epoch 953/1000\n",
      "9/9 [==============================] - 1s 85ms/step - loss: 1.9809 - accuracy: 0.3084 - val_loss: 6.2231 - val_accuracy: 0.0822\n",
      "Epoch 954/1000\n",
      "9/9 [==============================] - 1s 100ms/step - loss: 1.9855 - accuracy: 0.3060 - val_loss: 6.2281 - val_accuracy: 0.0822\n",
      "Epoch 955/1000\n",
      "9/9 [==============================] - 1s 86ms/step - loss: 2.0248 - accuracy: 0.2970 - val_loss: 6.2294 - val_accuracy: 0.0822\n",
      "Epoch 956/1000\n",
      "9/9 [==============================] - 1s 100ms/step - loss: 1.9925 - accuracy: 0.3097 - val_loss: 6.2327 - val_accuracy: 0.0819\n",
      "Epoch 957/1000\n",
      "9/9 [==============================] - 1s 96ms/step - loss: 1.9822 - accuracy: 0.3034 - val_loss: 6.2312 - val_accuracy: 0.0822\n",
      "Epoch 958/1000\n",
      "9/9 [==============================] - 1s 97ms/step - loss: 2.0400 - accuracy: 0.3056 - val_loss: 6.2420 - val_accuracy: 0.0822\n",
      "Epoch 959/1000\n",
      "9/9 [==============================] - 1s 121ms/step - loss: 2.0185 - accuracy: 0.3039 - val_loss: 6.2388 - val_accuracy: 0.0828\n",
      "Epoch 960/1000\n",
      "9/9 [==============================] - 1s 93ms/step - loss: 2.0150 - accuracy: 0.3068 - val_loss: 6.2402 - val_accuracy: 0.0825\n",
      "Epoch 961/1000\n",
      "9/9 [==============================] - 1s 108ms/step - loss: 1.9627 - accuracy: 0.3082 - val_loss: 6.2445 - val_accuracy: 0.0816\n",
      "Epoch 962/1000\n",
      "9/9 [==============================] - 1s 104ms/step - loss: 2.0189 - accuracy: 0.3026 - val_loss: 6.2481 - val_accuracy: 0.0819\n",
      "Epoch 963/1000\n",
      "9/9 [==============================] - 1s 86ms/step - loss: 1.9933 - accuracy: 0.3033 - val_loss: 6.2473 - val_accuracy: 0.0822\n",
      "Epoch 964/1000\n",
      "9/9 [==============================] - 1s 105ms/step - loss: 1.9939 - accuracy: 0.3088 - val_loss: 6.2531 - val_accuracy: 0.0822\n",
      "Epoch 965/1000\n",
      "9/9 [==============================] - 1s 131ms/step - loss: 1.9984 - accuracy: 0.3075 - val_loss: 6.2523 - val_accuracy: 0.0819\n",
      "Epoch 966/1000\n",
      "9/9 [==============================] - 1s 88ms/step - loss: 1.9800 - accuracy: 0.3063 - val_loss: 6.2520 - val_accuracy: 0.0833\n",
      "Epoch 967/1000\n",
      "9/9 [==============================] - 1s 109ms/step - loss: 1.9923 - accuracy: 0.3076 - val_loss: 6.2632 - val_accuracy: 0.0816\n",
      "Epoch 968/1000\n",
      "9/9 [==============================] - 1s 101ms/step - loss: 2.0167 - accuracy: 0.3087 - val_loss: 6.2614 - val_accuracy: 0.0822\n",
      "Epoch 969/1000\n",
      "9/9 [==============================] - 1s 115ms/step - loss: 2.0040 - accuracy: 0.3030 - val_loss: 6.2615 - val_accuracy: 0.0816\n",
      "Epoch 970/1000\n",
      "9/9 [==============================] - 1s 120ms/step - loss: 1.9946 - accuracy: 0.3034 - val_loss: 6.2676 - val_accuracy: 0.0828\n",
      "Epoch 971/1000\n",
      "9/9 [==============================] - 1s 94ms/step - loss: 1.9999 - accuracy: 0.3029 - val_loss: 6.2690 - val_accuracy: 0.0825\n",
      "Epoch 972/1000\n",
      "9/9 [==============================] - 1s 101ms/step - loss: 2.0198 - accuracy: 0.3042 - val_loss: 6.2733 - val_accuracy: 0.0825\n",
      "Epoch 973/1000\n",
      "9/9 [==============================] - 1s 106ms/step - loss: 1.9899 - accuracy: 0.3097 - val_loss: 6.2753 - val_accuracy: 0.0828\n",
      "Epoch 974/1000\n",
      "9/9 [==============================] - 1s 96ms/step - loss: 1.9877 - accuracy: 0.3048 - val_loss: 6.2771 - val_accuracy: 0.0825\n",
      "Epoch 975/1000\n",
      "9/9 [==============================] - 1s 100ms/step - loss: 2.0120 - accuracy: 0.3029 - val_loss: 6.2751 - val_accuracy: 0.0828\n",
      "Epoch 976/1000\n",
      "9/9 [==============================] - 1s 109ms/step - loss: 1.9911 - accuracy: 0.3069 - val_loss: 6.2841 - val_accuracy: 0.0828\n",
      "Epoch 977/1000\n",
      "9/9 [==============================] - 1s 105ms/step - loss: 1.9745 - accuracy: 0.3098 - val_loss: 6.2801 - val_accuracy: 0.0828\n",
      "Epoch 978/1000\n",
      "9/9 [==============================] - 1s 130ms/step - loss: 1.9836 - accuracy: 0.3109 - val_loss: 6.2878 - val_accuracy: 0.0825\n",
      "Epoch 979/1000\n",
      "9/9 [==============================] - 1s 108ms/step - loss: 1.9880 - accuracy: 0.3083 - val_loss: 6.2840 - val_accuracy: 0.0825\n",
      "Epoch 980/1000\n",
      "9/9 [==============================] - 1s 119ms/step - loss: 1.9539 - accuracy: 0.3113 - val_loss: 6.2938 - val_accuracy: 0.0825\n",
      "Epoch 981/1000\n",
      "9/9 [==============================] - 1s 101ms/step - loss: 1.9570 - accuracy: 0.3139 - val_loss: 6.2900 - val_accuracy: 0.0833\n",
      "Epoch 982/1000\n",
      "9/9 [==============================] - 1s 123ms/step - loss: 1.9932 - accuracy: 0.3051 - val_loss: 6.2953 - val_accuracy: 0.0828\n",
      "Epoch 983/1000\n",
      "9/9 [==============================] - 1s 98ms/step - loss: 1.9871 - accuracy: 0.3049 - val_loss: 6.2998 - val_accuracy: 0.0830\n",
      "Epoch 984/1000\n",
      "9/9 [==============================] - 1s 122ms/step - loss: 1.9478 - accuracy: 0.3073 - val_loss: 6.3042 - val_accuracy: 0.0830\n",
      "Epoch 985/1000\n",
      "9/9 [==============================] - 1s 102ms/step - loss: 1.9510 - accuracy: 0.3111 - val_loss: 6.3015 - val_accuracy: 0.0822\n",
      "Epoch 986/1000\n",
      "9/9 [==============================] - 1s 111ms/step - loss: 1.9603 - accuracy: 0.3121 - val_loss: 6.3051 - val_accuracy: 0.0828\n",
      "Epoch 987/1000\n",
      "9/9 [==============================] - 1s 105ms/step - loss: 1.9699 - accuracy: 0.3116 - val_loss: 6.3080 - val_accuracy: 0.0833\n",
      "Epoch 988/1000\n",
      "9/9 [==============================] - 1s 116ms/step - loss: 1.9880 - accuracy: 0.3140 - val_loss: 6.3109 - val_accuracy: 0.0830\n",
      "Epoch 989/1000\n",
      "9/9 [==============================] - 1s 114ms/step - loss: 1.9688 - accuracy: 0.3063 - val_loss: 6.3138 - val_accuracy: 0.0825\n",
      "Epoch 990/1000\n",
      "9/9 [==============================] - 1s 122ms/step - loss: 1.9703 - accuracy: 0.3104 - val_loss: 6.3143 - val_accuracy: 0.0830\n",
      "Epoch 991/1000\n",
      "9/9 [==============================] - 1s 94ms/step - loss: 1.9756 - accuracy: 0.3112 - val_loss: 6.3220 - val_accuracy: 0.0822\n",
      "Epoch 992/1000\n",
      "9/9 [==============================] - 1s 104ms/step - loss: 1.9700 - accuracy: 0.3147 - val_loss: 6.3238 - val_accuracy: 0.0830\n",
      "Epoch 993/1000\n",
      "9/9 [==============================] - 1s 105ms/step - loss: 1.9807 - accuracy: 0.3144 - val_loss: 6.3273 - val_accuracy: 0.0825\n",
      "Epoch 994/1000\n",
      "9/9 [==============================] - 1s 96ms/step - loss: 1.9682 - accuracy: 0.3108 - val_loss: 6.3284 - val_accuracy: 0.0822\n",
      "Epoch 995/1000\n",
      "9/9 [==============================] - 1s 106ms/step - loss: 1.9684 - accuracy: 0.3110 - val_loss: 6.3330 - val_accuracy: 0.0825\n",
      "Epoch 996/1000\n",
      "9/9 [==============================] - 1s 96ms/step - loss: 1.9791 - accuracy: 0.3084 - val_loss: 6.3300 - val_accuracy: 0.0830\n",
      "Epoch 997/1000\n",
      "9/9 [==============================] - 1s 116ms/step - loss: 1.9883 - accuracy: 0.3127 - val_loss: 6.3454 - val_accuracy: 0.0816\n",
      "Epoch 998/1000\n",
      "9/9 [==============================] - 1s 95ms/step - loss: 2.0391 - accuracy: 0.3032 - val_loss: 6.3360 - val_accuracy: 0.0822\n",
      "Epoch 999/1000\n",
      "9/9 [==============================] - 1s 107ms/step - loss: 1.9908 - accuracy: 0.3117 - val_loss: 6.3441 - val_accuracy: 0.0828\n",
      "Epoch 1000/1000\n",
      "9/9 [==============================] - 1s 91ms/step - loss: 1.9811 - accuracy: 0.3119 - val_loss: 6.3401 - val_accuracy: 0.0822\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f79e9432f70>"
      ]
     },
     "metadata": {},
     "execution_count": 88
    }
   ],
   "source": [
    "z = np.zeros((len(input_sequences),LATENT_DIM))\n",
    "model.fit(\n",
    "    [input_sequences, z, z],\n",
    "    one_hot_targets,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    validation_split=VALIDATION_SPLIT,\n",
    "    epochs = EPOCHS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "input2 = Input(shape=(1,))\n",
    "x = embedding_layer(input2)\n",
    "x, h, c = lstm(x, initial_state=[initial_h, intial_c])\n",
    "output2 = dense(x)\n",
    "sampling_model = Model([input2, initial_h, intial_c], [output2, h, c])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx2word = {v:k for k,v in word2idx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_line():\n",
    "    # initial inputs\n",
    "    np_input = np.array([[word2idx['<sos>']]])\n",
    "    h = np.zeros((1, LATENT_DIM))\n",
    "    c = np.zeros((1, LATENT_DIM))\n",
    "\n",
    "    # so we know when to quit\n",
    "    eos = word2idx['<eos>']\n",
    "\n",
    "    # store the output here\n",
    "    output_sentence = []\n",
    "\n",
    "    for _ in range(max_sequence_length):\n",
    "        o, h, c = sampling_model.predict([np_input, h, c])\n",
    "\n",
    "        # print(\"o.shape:\", o.shape, o[0,0,:10])\n",
    "        # print(o[0,0])\n",
    "        # idx = np.argmax(o[0,0])\n",
    "        probs = o[0, 0]\n",
    "        if np.argmax(probs) == 0:\n",
    "            print(\"wtf\")\n",
    "        probs[0] = 0\n",
    "        # print(probs[0:10])\n",
    "        probs /= probs.sum()\n",
    "        idx = np.random.choice(len(probs), p=probs)\n",
    "        if idx == eos:\n",
    "            break\n",
    "\n",
    "        # accuulate output\n",
    "        output_sentence.append(idx2word.get(idx, '<WTF %s>' % idx))\n",
    "\n",
    "        # make the next input into model\n",
    "        np_input[0, 0] = idx\n",
    "\n",
    "    return ' '.join(output_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "now not left's no more the bones his abode.\n",
      "but to-night so like the more for the novelty,\n",
      "then know from them as concerned for a sudden movement toward the\n",
      "sitting or three?'\n",
      "my mother left them so, they're to be.' for adventurously.' here\n",
      "i'll tell you happen to reckon the bones.'\n",
      "and tell with one for voices.' of youthful faces.\n",
      "nor granny's, surely. call i left all halted too,\n",
      "there'd been all over me john said my lofty can accept 'it's\n",
      "but if in me in some publisher\n"
     ]
    }
   ],
   "source": [
    "for _ in range(10):\n",
    "    print(sample_line())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}