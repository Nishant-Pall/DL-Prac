{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit"
  },
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import keras.backend as K\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import tarfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Dense, Embedding, Input, Lambda, Reshape, add, dot, Activation\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.optimizers import rmsprop_v2\n",
    "from keras.utils.data_utils import get_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Downloading data from https://s3.amazonaws.com/text-datasets/babi_tasks_1-20_v1-2.tar.gz\n",
      "11747328/11745123 [==============================] - 52s 4us/step\n",
      "11755520/11745123 [==============================] - 52s 4us/step\n"
     ]
    }
   ],
   "source": [
    "path = get_file(\n",
    "    'babi-tasks-v1-2.tar.gz',\n",
    "    origin='https://s3.amazonaws.com/text-datasets/babi_tasks_1-20_v1-2.tar.gz')\n",
    "tar = tarfile.open(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "challenges = {\n",
    "  # QA1 with 10,000 samples\n",
    "    'single_supporting_fact_10k': 'tasks_1-20_v1-2/en-10k/qa1_single-supporting-fact_{}.txt',\n",
    "    # QA2 with 10,000 samples\n",
    "    'two_supporting_facts_10k': 'tasks_1-20_v1-2/en-10k/qa2_two-supporting-facts_{}.txt',   \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(sent):\n",
    "    '''\n",
    "    Return the tokens of a sentence including punctuation\n",
    "     >>> tokenize('Bob dropped the apple. Where is the apple?')\n",
    "    ['Bob', 'dropped', 'the', 'apple', '.', 'Where', 'is', 'the',          'apple', '?']\n",
    "  \n",
    "    '''\n",
    "\n",
    "    return [x.strip() for x in re.split('(\\W+)', sent) if x.strip()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stories(f):\n",
    "    # data will return a list of triples\n",
    "    # each triple contains:\n",
    "    #   1. a story\n",
    "    #   2. a question about the story\n",
    "    #   3. the answer to the question\n",
    "    data = []\n",
    "\n",
    "    # use this list to keep track of the story so far\n",
    "    story = []\n",
    "\n",
    "    # print a random story, helpful to see the data\n",
    "    printed = False\n",
    "    for line in f:\n",
    "        line = line.decode('utf-8').strip()\n",
    "\n",
    "        # split the line number from the rest of the line\n",
    "        nid, line = line.split(' ', 1)\n",
    "\n",
    "        # see if we should begin a new story\n",
    "        if int(nid) == 1:\n",
    "            story = []\n",
    "\n",
    "        # this line contains a question and answer if it has a tab\n",
    "        #       question<TAB>answer\n",
    "        # it also tells us which line in the story is relevant to the answer\n",
    "        # Note: we actually ignore this fact, since the model will learn\n",
    "        #       which lines are important\n",
    "        # Note: the max line number is not the number of lines of the story\n",
    "        #       since lines with questions do not contain any story\n",
    "        # one story may contain MULTIPLE questions\n",
    "        if '\\t' in line:\n",
    "            q, a, supporting = line.split('\\t')\n",
    "            q = tokenize(q)\n",
    "\n",
    "            # numbering each line is very useful\n",
    "            # it's the equivalent of adding a unique token to the front\n",
    "            # of each sentence\n",
    "            story_so_far = [[str(i)] + s for i, s in enumerate(story) if s]\n",
    "\n",
    "            # uncomment if you want to see what a story looks like\n",
    "            # if not printed and np.random.rand() < 0.5:\n",
    "            #     print(\"story_so_far:\", story_so_far)\n",
    "            #     printed = True\n",
    "            data.append((story_so_far, q, a))\n",
    "            story.append('')\n",
    "        else:\n",
    "            # just add the line to the current story\n",
    "            story.append(tokenize(line))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recursively flatten a list\n",
    "def should_flatten(el):\n",
    "    return not isinstance(el, (str, bytes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(l):\n",
    "    for el in l:\n",
    "        # if el is not a string, yield it from flatten\n",
    "        if should_flatten(el):\n",
    "            yield from flatten(el)\n",
    "        else:\n",
    "            yield el\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert stories from words into lists of word indexes(integers)\n",
    "# pad each sequence so that they are the same length\n",
    "# we will need to re-pad the stories later so that\n",
    "# each story is the same length\n",
    "\n",
    "def vectorize_stories(data, word2idx, story_maxlen, query_maxlen):\n",
    "    inputs, queries, answers = [], [], []\n",
    "    for story, query, answer in data:\n",
    "        inputs.append([[word2idx[w] for w in s] for s in story])\n",
    "        queries.append([word2idx[w] for w in query])\n",
    "        answers.append([word2idx[answer]])\n",
    "\n",
    "    return (\n",
    "        [pad_sequences(x, maxlen=story_maxlen) for x in inputs],\n",
    "        pad_sequences(queries, maxlen=query_maxlen),\n",
    "        np.array(answers)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is like 'pad_sequences' but for entire stories\n",
    "# we are padding each story with zeros so every story\n",
    "# has the same number of sentences\n",
    "# append an array of zeros of size:\n",
    "# (max_sentences - num sentences in story, max words in sentence)\n",
    "def stack_inputs(inputs, story_maxsents, story_maxlen):\n",
    "    for i, story in enumerate(inputs):\n",
    "        inputs[i] = np.concatenate(\n",
    "            [\n",
    "                story,\n",
    "                np.zeros(\n",
    "                    (story_maxsents - story.shape[0], story_maxlen),\n",
    "                    'int'\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "    return np.stack(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a function to get the data since\n",
    "# we want to load both the single supporting fact data\n",
    "# and the two supporting fact data later\n",
    "\n",
    "def get_data(challenge_type):\n",
    "    # input should either be 'single_supporting_fact_10k' or 'two_supporting_facts_10k'\n",
    "    challenge = challenges[challenge_type]\n",
    "\n",
    "    # return a list of triplets\n",
    "    # (story, ques, ans)\n",
    "\n",
    "    train_stories = get_stories(tar.extractfile(challenge.format('train')))\n",
    "    test_stories = get_stories(tar.extractfile(challenge.format('test')))\n",
    "\n",
    "    # group all stories together\n",
    "    stories = train_stories + test_stories\n",
    "\n",
    "    # so we can get the max length of each story, of each sentence, and of each question\n",
    "    story_maxlen = max((len(s) for x, _, _ in stories for s in x))\n",
    "    story_maxsents = max((len(x) for x, _, _ in stories))\n",
    "    query_maxlen = max(len(x) for _, x, _ in stories)\n",
    "\n",
    "    # Create vocabulary of corpus and find size, including a padding element\n",
    "    vocab = sorted(set(flatten(stories)))\n",
    "    vocab.insert(0, '<PAD>')\n",
    "    vocab_size = len(vocab)\n",
    "\n",
    "    # Create index mapping for the vocab\n",
    "    word2idx = {c: i for i, c in enumerate(vocab)}\n",
    "\n",
    "    # convert stories from strings to lists of integers\n",
    "    input_train, queries_train, answers_train = vectorize_stories(\n",
    "        train_stories,\n",
    "        word2idx,\n",
    "        story_maxlen,\n",
    "        query_maxlen\n",
    "    )\n",
    "    input_test, queries_test, answers_test = vectorize_stories(\n",
    "        test_stories,\n",
    "        word2idx,\n",
    "        story_maxlen,\n",
    "        query_maxlen\n",
    "    )\n",
    "\n",
    "    # convert inputs into 3-D numpy arrays\n",
    "    inputs_train = stack_inputs(input_train, story_maxsents, story_maxlen)\n",
    "    inputs_test = stack_inputs(input_test, story_maxsents, story_maxlen)\n",
    "    print(f'inputs_train.shape : {inputs_train.shape}, inputs_test.shape: {inputs_test.shape}' )\n",
    "\n",
    "    # return model inputs for keras\n",
    "\n",
    "    return train_stories, test_stories, \\\n",
    "        inputs_train, queries_train, answers_train, \\\n",
    "        inputs_test, queries_test, answers_test, \\\n",
    "        story_maxsents, story_maxlen, query_maxlen, \\\n",
    "        vocab, vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "inputs_train.shape : (10000, 10, 8), inputs_test.shape: (1000, 10, 8)\n"
     ]
    }
   ],
   "source": [
    "# get the single supporting fact data\n",
    "train_stories, test_stories, \\\n",
    "    inputs_train, queries_train, answers_train, \\\n",
    "    inputs_test, queries_test, answers_test, \\\n",
    "    story_maxsents, story_maxlen, query_maxlen, \\\n",
    "    vocab, vocab_size = get_data('single_supporting_fact_10k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}